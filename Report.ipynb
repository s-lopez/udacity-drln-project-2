{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous control project\n",
    "This notebook describes the implementation and illustrates the live performance of the Reacher agent in the actual environment.\n",
    "\n",
    "# Background\n",
    "#### Deep Deterministic Policy Gradients (DDPG)\n",
    "DDPGs are a form of actor-critic methods that are applicable to continuous action space states, in contrast to other algorithms like Deep Q Networks. The original description of DDPGs can be found [here](https://arxiv.org/pdf/1509.02971.pdf). DDPGs train simultaneously two networks: An actor, which selects the optimal (deterministic) policy based on the current state, and a critic, which approximates the value function of the state-action pair.\n",
    "\n",
    "Both the actor and the critic are cloned, much like in DQN networks. To update the networks, the outputs of the cloned networks are used.\n",
    "\n",
    "# Parameters\n",
    "#### Actor\n",
    "The actor network has three layers, with sizes `33` (input), `240`, `120` and `4` (output). Batch normalization is applied after the first layer. ReLUs are applied after the first and second layer. A tanh function is appliead after the last layer to ensure outputs in the range `[-1, 1]`. The actor applies noise to its actions as a Ornstein-Uhlenbeck process with $\\mu=0$, $\\theta=0.05$ and $\\sigma=0.02$\n",
    "\n",
    "#### Critic\n",
    "The critic network has a first layer of sizes `33`, `240`, after wich batch normalization and a ReLU are applied. To this output we append `4` additional inputs (equivalent to the action space) as an input to layer 2, with an output size of `120` and a ReLU at its end. The last layer has an output size of `1`.\n",
    "\n",
    "#### Learning\n",
    "This implementation uses the following learning parameters\n",
    "* A replay buffer of size `10⁶` is used\n",
    "* The mini-batch size is `256`\n",
    "* Discount rate $\\gamma=0.99$\n",
    "* Soft update rate $\\tau=10³$\n",
    "* Learning rate $\\eta=10^{⁻4}$ (for both networks) \n",
    "* Weight decay `= 0`\n",
    "* The critic's gradient is clipped to the range \\[-1, 1\\]\n",
    "\n",
    "# Results\n",
    "\n",
    "![Results](media/agent-performance-v0.png)\n",
    "\n",
    "The DDPG solves the environment after 287 episodes.\n",
    "\n",
    "# Improvements\n",
    "\n",
    "The following are suggestions on how to improve the performance.\n",
    "* **Rewrite the replay buffer.** The replay buffer is implemented as a python deque, which performs fast queue/deque operations, but is [inefficient (O(n))](https://docs.python.org/3/library/collections.html#collections.deque) when sampling from the middle of the buffer. Having a faster replay buffer means faster trainings and more time for experiments.\n",
    "* **Perform a hyperparameter search.** There is guaranteed a better set of hyperparameters available, which would also speed up learning.\n",
    "* **Implement multiple agents.** Training multiple agents in parallel has been shown to speed up training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent demonstration\n",
    "Below we will see the trained agents following a greedy policy in the actual environment.\n",
    "\n",
    "**Note:** Running the cell under \"Initialization\" will open a full-screen Unity window. Switch windows or run the whole script at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# Invite our agent & import utils\n",
    "from ddpg_agent import Agent\n",
    "#from random import random as rnd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "PATH_TO_ENV = \"Reacher_Linux/Reacher.x86\"\n",
    "BRAIN = \"ReacherBrain\"\n",
    "TRAINING = False\n",
    "\n",
    "env = UnityEnvironment(file_name=PATH_TO_ENV, no_graphics=TRAINING)\n",
    "\n",
    "ACTION_SIZE = env.brains[BRAIN].vector_action_space_size\n",
    "STATE_SIZE = env.brains[BRAIN].vector_observation_space_size\n",
    "\n",
    "def act(env, actions, brain_name=BRAIN) -> tuple:\n",
    "    \"\"\"Sends actions to the environment env and observes the results.\n",
    "    Returns a tuple of rewards, next_states, dones (One per agent)\"\"\"\n",
    "    action_result = env.step(actions)[brain_name] # Act on the environment and observe the result\n",
    "    return (action_result.rewards,\n",
    "            action_result.vector_observations, # next states\n",
    "            action_result.local_done) # True if the episode ended\n",
    "    \n",
    "def reset(env, training=TRAINING, brain_name=BRAIN) -> np.ndarray:\n",
    "    \"\"\"Syntactic sugar for resetting the unity environment\"\"\"\n",
    "    return env.reset(train_mode=training)[brain_name].vector_observations\n",
    "\n",
    "def visualize(agent, env): \n",
    "    states = reset(env)\n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = agent.decide(states)      # Choose an action based on the state\n",
    "        rewards, next_states, dones = act(env, actions)    # Send the action to the environment\n",
    "        score += rewards[0]                                # Update the score\n",
    "        states = next_states                             # Roll over the state to next time step\n",
    "        done = any(dones)\n",
    "    print(\"Score: {}\".format(score))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(STATE_SIZE, ACTION_SIZE, 0)\n",
    "state_dict = torch.load('models/30.0-289-27.12.19_18.05-checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actor_local.load_state_dict(state_dict['actor_state_dict'])\n",
    "agent.critic_local.load_state_dict(state_dict['critic_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 36.699999179691076\n"
     ]
    }
   ],
   "source": [
    "visualize(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuts down the Unity environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
