{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_ENV = \"Reacher_Linux/Reacher.x86\"\n",
    "BRAIN = \"ReacherBrain\"\n",
    "TRAINING = True\n",
    "\n",
    "env = UnityEnvironment(file_name=PATH_TO_ENV, no_graphics=TRAINING)\n",
    "\n",
    "ACTION_SIZE = env.brains[BRAIN].vector_action_space_size\n",
    "STATE_SIZE = env.brains[BRAIN].vector_observation_space_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[BRAIN]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "print('Size of each action:', ACTION_SIZE)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], STATE_SIZE))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(env, actions, brain_name=BRAIN) -> tuple:\n",
    "    \"\"\"Sends actions to the environment env and observes the results.\n",
    "    Returns a tuple of rewards, next_states, dones (One per agent)\"\"\"\n",
    "    action_result = env.step(actions)[brain_name] # Act on the environment and observe the result\n",
    "    return (action_result.rewards,\n",
    "            action_result.vector_observations, # next states\n",
    "            action_result.local_done) # True if the episode ended\n",
    "    \n",
    "def reset(env, training=TRAINING, brain_name=BRAIN) -> np.ndarray:\n",
    "    \"\"\"Syntactic sugar for resetting the unity environment\"\"\"\n",
    "    return env.reset(train_mode=training)[brain_name].vector_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import clear_output\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling (optional)\n",
    "import cProfile\n",
    "profiling = False # Set to True to profile execution time\n",
    "if profiling:\n",
    "    profile = cProfile.Profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invite our agent & import utils\n",
    "from ddpg_agent import Agent\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, num_episodes, log_freq, max_t=20000, profiling=False, profile=None):        \n",
    "    score_hist = np.zeros((num_agents, 1))\n",
    "    if profiling: \n",
    "        profile.enable()\n",
    "    \n",
    "    # Train\n",
    "    for i in range(num_episodes):\n",
    "        states = reset(env)\n",
    "        scores = np.zeros(num_agents)\n",
    "        for j in range(max_t): # Safer than while\n",
    "            # Decide\n",
    "            state = states.squeeze() # One agent only\n",
    "            action = agent.decide(state)  # Choose an action based on the state\n",
    "            actions = np.expand_dims(action, 0) # One agent only\n",
    "            # Act\n",
    "            rewards, next_states, dones = act(env, actions)     # Send the actions to the environment\n",
    "            scores += rewards                         # update the score (for each agent)\n",
    "            # Learn\n",
    "            agent.step(state, action, rewards[0], next_states[0], dones[0]) # Learn step\n",
    "            # Step\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            # Exit\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break       \n",
    "        # Logging\n",
    "        score_hist = np.concatenate((score_hist, scores[:, None]), axis=1)\n",
    "        if (i % log_freq == 0) and (i > 0):\n",
    "            print(f'Avg. score episodes {i-log_freq+1}-{i+1}: {np.mean(score_hist.squeeze()[-log_freq:])}')\n",
    "\n",
    "    if profiling: \n",
    "        profile.disable()\n",
    "        \n",
    "    return score_hist[1:].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "agent = Agent(STATE_SIZE, ACTION_SIZE, np.random.randint(1e5), cuda=False)\n",
    "score_hist = train(agent, env, num_episodes=1000, log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting + visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(signal:np.ndarray , period: int):\n",
    "    buffer = [np.nan] * period\n",
    "    for i in range(period,len(signal)):\n",
    "        buffer.append(signal[i-period:i].mean())\n",
    "    return np.array(buffer)\n",
    "\n",
    "#def moving_std(signal:np.ndarray , period: int):\n",
    "#    buffer = [np.nan] * period\n",
    "#    for i in range(period,len(signal)):\n",
    "#        buffer.append(signal[i-period:i].std())\n",
    "#    return np.array(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "avg_scores = moving_average(score_hist, length)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(avg_scores_vanilla, label='avg_scores')\n",
    "plt.hlines(13, xmin=0, xmax=1500, colors='r', label='solved')\n",
    "plt.grid()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Avg. score')\n",
    "plt.title(f'Avg. score over {length} episodes')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim((0, 1400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agent(agent, name, params):\n",
    "    \"\"\"Saves the agent's parameters and the underlying pytorch model\"\"\"\n",
    "    checkpoint = {'state_dict': agent.qnetwork_local.state_dict(),\n",
    "                  'agent_params': params}\n",
    "    torch.save(checkpoint, f'models/{name}-checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_agent(bananator_vanilla, 'vanilla', params_vanilla)\n",
    "save_agent(bananator_per, 'per', params_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
